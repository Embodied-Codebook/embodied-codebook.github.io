<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Selective Visual Representations Improve Convergence and Generalization for Embodied AI">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Embodied AI Codebook</title>
<!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script> -->

<!--   <script>
     -->



  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    /* Define a CSS class for your section with a specific background color */
    .colored-section {
      background-color: #F8F8F8; /* Replace with your desired color code or name */
      padding: 20px; /* Optional: Add padding for better spacing */
    }

    .colored-section2 {
      background-color: #f3ebde; /* Replace with your desired color code or name */
      padding: 20px; /* Optional: Add padding for better spacing */
    }

    .colored-section3 {
      background-color: #f0eeee; /* Replace with your desired color code or name */
      padding: 20px; /* Optional: Add padding for better spacing */
    }

    ol {
      margin-left: 50px; /* Adjust the value to control the tab size */
    }
  </style>

  <style>
    .image-container {
      display: inline-block;
      margin: 10px; /* Add margin for spacing */
    }

    .equal-height {
      height: 250px; /* Set the desired equal height for both images */
    }
  </style>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>



</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Selective Visual Representations Improve Convergence and Generalization for Embodied-AI</h1>
<!--          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://2023.emnlp.org/">Under Review</a></h3>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a target="_blank" href="https://ainaz99.github.io/">Ainaz Eftekhar*</a><sup>1,2</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://kuohaozeng.github.io//">Kuo-Hao Zeng*</a><sup>2</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://duanjiafei.com/">Jiafei Duan</a><sup>1,2</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://homes.cs.washington.edu/~ali//">Ali Farhadi</a><sup>1, 2</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://anikem.github.io//">Ani Kembhavi</a><sup>1, 2</sup>,</span>
            <span class="author-block"><a target="_blank" href="http://www.ranjaykrishna.com/index.html/">Ranjay Krishna</a><sup>1,2</sup></span>
          </div>



          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Allen Institute for Artifical Intelligence</span>
          </div>


          <h3 class="author-block2"> *Equal Contribution</h3>

          <br><br>



          <span class="link-block"><a target="_blank" href="https://arxiv.org/abs/2311.04193" class="external-link
          button is-normal is-rounded is-dark"><span class="icon"><i class="fas fa-file"></i></span><span>ArXiv</span></a></span>

          <!-- Code Link. -->
          <span class="link-block"><a target="_blank" href="https://embodied-codebook.github.io/" class="external-link
          button is-normal is-rounded is-dark"><span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon)</span></a></span>
<!--          <span class="link-block">-->
<!--          <a target="_blank" href="" class="external-link button is-normal is-rounded is-dark">-->
<!--            <span class="icon">-->
<!--              <i class="fas fa-database" aria-hidden="true"></i>-->
<!--            </span>-->
<!--            <span>Dataset</span>-->
<!--          </a>-->
<!--        </span>-->
          <br><br>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/teaser_v2.png" class="interpolation-image" alt="Interpolate start reference image." />

        </div>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf2">Inspired by <i>selective attention</i> in humans—the process through which people filter
            their perception based on their experiences, knowledge, and the task at hand—we introduce a
            parameter-efficient approach to filter visual stimuli for Embodied-AI.
        </span>
        </h2>
      </div>
    </div>
  </div>
</section>


<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-steve">-->
<!--          <img src="media/procthor1_baseline.png">-->
<!--          <img src="media/procthor1_baseline.gif" alt="Your GIF Image">-->
<!--        </div>-->
<!--        <div class="item item-chair-tp">-->
<!--          <img src="media/procthor2_baseline.png">-->
<!--          <img src="media/procthor2_baseline.gif" alt="Your GIF Image">-->
<!--        </div>-->
<!--         <div class="item item-shiba">-->
<!--          <img src="media/procthor3_baseline.png">-->
<!--          <img src="media/procthor1_baseline.gif" alt="Your GIF Image">-->
<!--        </div>-->
<!--        <div class="item item-fullbody">-->
<!--          <video poster="" id="fullbody" autoplay muted loop height="100%">-->
<!--            <source src="media/demo.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-shiba">-->
<!--          <video poster="" id="shiba" autoplay muted loop height="100%">-->
<!--            <source src="media/demo.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-blueshirt">-->
<!--          <video poster="" id="blueshirt" autoplay muted loop height="100%">-->
<!--            <source src="media/demo.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-shiba">-->
<!--          <video poster="" id="shiba" autoplay muted loop height="100%">-->
<!--            <source src="media/demo.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--       <div class="item item-chair-tp">-->
<!--          <video poster="" id="chair-tp" autoplay muted loop height="100%">-->
<!--            <source src="media/demo.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--         <div class="item item-chair-tp">-->
<!--          <video poster="" id="chair-tp" autoplay muted loop height="100%">-->
<!--            <source src="media/demo.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->





<!--<section class="hero teaser">-->
<!--  <div class="container is-fullhd">-->
<!--    <div class="hero-body">-->
<!--      <div class="container">-->
<!--        <div class="columns is-vcentered  is-centered">-->
<!--          <video id="teaser" autoplay muted loop height="100%">-->
<!--            <source src="media/demo.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          </br>-->
<!--        </div>-->
<!--        <br>-->
<!--        <h2 class="subtitle has-text-centered">-->
<!--        <span class="dperact">AR2-D2's</span> demonstrations collection interface via an iPhone/iPad.-->
<!--        </h2>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->





<section class="colored-section">
  <div class="container is-max-desktop is-full-fullhd">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <br/>
          <p>
            Embodied-AI models often employ off the shelf vision backbones like CLIP to encode their visual observations.
            Although such general purpose representations encode rich syntactic and semantic information about the scene,
            much of this information is often irrelevant to the specific task at hand. This introduces noise within the
            learning process and distracts the agent's focus from task-relevant visual cues.
            Inspired by selective attention in humans—the process through which people filter their perception based
            on their experiences, knowledge, and the task at hand—we introduce a parameter-efficient approach to filter
            visual stimuli for embodied-AI. Our approach induces a task-conditioned bottleneck using a small learnable
            codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned
            selective filter over the visual observation. Our experiments showcase state-of-the-art performance for
            <i>Object Goal Navigation</i> and <i>Object Displacement</i> across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR,
            AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able to
            generalize better and converge faster when adapted to other simulation environments such as Habitat.
            Our qualitative analyses show that agents explore their environments more effectively and their
            representations retain task-relevant information like target object recognition while ignoring superfluous
            information about other objects.
          </p>
        </div>
      </div>
    </div>
    <br>

    <!--/ Abstract. -->

  </div>


</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">

<style>
  .centered-container {
    display: flex;
    flex-direction: column;
    align-items: center;
    text-align: center;
  }

  .interpolation-image {
    display: block;
    margin: 20px 0;  /* Give some space above and below the image */
  }
</style>


<section>
  <div class="rows">
    <div class="row is-full-width">
      <h2 class="title is-3"><span class="dperact">The Codebook Module</span></h2>
      <h3 class="title is-4">A Filtering Mechanism of Visual Representations for Embodied-AI</h3>
      <p>
        Conventional embodied-AI frameworks usually employ general-purpose visual backbones like CLIP to extract the
        visual representations from the input. Such representations capture an abundance of details and a
        significant amount of task-irrelevant information.
        For example, to find a specific object in a house, the agent doesn't need to know about other distractor
        objects in the agent’s view, about their colors, materials, attributes, etc. These distractions introduce
        unnecessary noise into the learning process, distracting the agent’s focus away from more pertinent visual cues.
        We draw from the substantial body of research in cognitive psychology to induce selective task-specific
        representations that filter irrelevant sensory input and only retain the necessary stimuli.
        <br>
        We introduce a compact learnable module that decouples the two objectives in embodied-AI tasks across different
        parameters in the network:
        <ol>
          <li> The input encoders and the codebook focus on extracting salient information for the task from the visual
            input</li>
          <li> Whereas the policy (RNN and actor-critic heads) can focus on decision-making based on this filtered
            information.</li>
        </ol>
      It acts as a task-conditioned bottleneck that filters out unnecessary information, allowing the agent to focus
      on more task-related visual cues.
      </p>

      <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/codebook_framework2.png" class="interpolation-image" alt="Interpolate start reference image."
           style="width: 70%;" />
      </div>


    </div>
  </div>
</section>

      <br><br>


<section>
  <div class="rows">
    <div class="row is-full-width">
      <h2 class="title is-3"><span class="dperact">Results</span></h2>
      <h3 class="title is-4 ">Codebook-Based Representations Improve Performance in Embodied-AI</h3>
      <p>
        Bottlenecking the task-conditioned embeddings using our
        codebook module results in significant improvements over the non-bottlenecked representations
        across a variety of Embodied-AI benchmarks. We consider <b>Object Goal Navigation</b> (<i>navigate to find a specific
        object category in a scene</i>) and <b>Object Displacement</b> (<i>bringing a source object to a destination
        object using a robotic arm</i>) across 5 benchmarks (ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR).
      </p>
      <br>

      <p>
      <b>Introducing New Metrics for Object Navigation.</b>
        We present <b>Curvature (k)</b> defined as


        <div>
            \[
            k = \frac{dx \times ddy - dy \times ddx}{(dx^2 + dy^2)^{\frac{3}{2}}}
            \]
        </div>

        as a key evaluation metric comparing the smoothness of the trajectories.
        Smoother trajectories are generally safer and more energy-efficient.
        <br> <br>

        We further introduce <b>Success Weighted by Episode Length (SEL)</b>,

        <div>
            \[
            \frac{1}{N} \sum_{i=1}^N S_i \frac{w_i}{max(w_i, e_i)}
            \]
        </div>

        as a substitute for
        <i>Success Weighted by Path Length (SPL)</i>. SPL is evaluated based on the <i>distance</i> traveled rather than the
        actual <i>number of steps taken</i> while the efficiency of a path should consider factors like time and energy
        consumption and therefore take into account actions such as rotations and look-ups/downs.
        <br> <br>

      </p>

      <title>Research Data Table</title>
      <style>
        table {
          margin: 0 auto;
          width: 70%;
          border-collapse: collapse;
        }
        table, th, td {
          border: 1px solid black;
        }
        th, td {
          padding: 8px;
          text-align: left;
        }
        th {
          background-color: #f2f2f2;
        }
      </style>
      </head>
      <body>

      <table>
        <tr>
          <th>Benchmark (Object Goal Navigation)</th>
          <th>Model</th>
          <th>SR(%) &#9650; </th>
          <th>EL &#9660; </th>
          <th>Curvature &#9660; </th>
<!--          <th>SPL &#9650; </th>-->
          <th>SEL &#9650; </th>
        </tr>
        <tr>
          <td>ProcTHOR-10k (test)</td>
          <td>EmbCLIP<br>+Codebook (Ours)</td>
          <td>67.70<br><b>73.72</b></td>
          <td>182.00<br><b>136.00</b></td>
          <td>0.58<br><b>0.23</b></td>
<!--          <td>49.00<br>48.37</td>-->
          <td>36.00<br><b>43.69</b></td>
        </tr>
        <tr>
          <td>ArchitecTHOR (0-shot)</td>
          <td>EmbCLIP<br>+Codebook (Ours)</td>
          <td>55.80<br><b>58.33</b></td>
          <td>222.00<br><b>174.00</b></td>
          <td>0.49<br><b>0.20</b></td>
<!--          <td>38.30<br>35.57</td>-->
          <td>20.57<br><b>28.31</b></td>
        </tr>
        <tr>
          <td>AI2-THOR (0-shot)</td>
          <td>EmbCLIP<br>+Codebook (Ours)</td>
          <td>70.00<br><b>78.40</b></td>
          <td>121.00<br><b>86.00</b></td>
          <td>0.29<br><b>0.16</b></td>
<!--          <td> 57.10<br>54.39</td>-->
          <td>21.45<br> <b>26.76</b></td>
        </tr>
        <tr>
          <td>RoboTHOR (0-shot)</td>
          <td>EmbCLIP<br>+Codebook (Ours)</td>
          <td>51.32<br><b>55.00</b></td>
          <td>-<br>-</td>
          <td>-<br>-</td>
<!--          <td><b>24.24</b><br>23.65</td>-->
          <td>-<br>-</td>
        </tr>
        <!-- Add more rows as needed -->
      </table>

      <br>

      <!-- Separate section for Object displacement if needed -->
      <table>
        <tr>
          <th>Benchmark (Object Displacement)</th>
          <th>Model</th>
          <th>PU(%) &#9650; </th>
          <th>SR(%) &#9650; </th>
        </tr>
        <tr>
          <td>ManipulaTHOR</td>
          <td>m-VOLE<br>+Codebook (Ours)</td>
          <td>81.20<br> <b>86.00</b></td>
          <td>59.60<br> <b>65.10</b></td>
        </tr>
        <!-- Add more rows as needed -->
      </table>

      <br><br>
      <h3 class="title is-4 ">Codebook-Bottlenecked Embedding is Easier to Transfer to New Visual Domains</h3>
      <p>
        The codebook-based embedding transfers across new visual domains without exhaustive fine-tuning.
        Our codebook bottleneck effectively decouples the process of learning salient visual information
        for the task from the process of decision-making based on this filtered information.
        Consequently, when faced with a similar task in a new visual domain, the need
        for adaptation is significantly reduced. In this scenario, only the modules responsible for extracting
        essential visual cues in the new domain require fine-tuning, while the decision-making modules
        can remain fixed. We show our ObjectNav agent trained in <b>AI2THOR</b> simulator can effectively adapt to the
        <b>Habitat</b> simulator (which is very different in visual characteristics, lighting, textures and other environmental
        factors) by merely fine-tuning a lightweight Adaptation Module.

      </p>

      <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/habitat_finetuning.png" class="interpolation-image" alt="Interpolate start reference image."
           style="width: 45%;" />
      </div>
      <h2 class="subtitle has-text-centered">
          <span class="dnerf2"><b>Lightweight Fine-tuning of the Adaptation Module.</b> We only finetune a few CNN
            layers, action and goal embedders, and the codebook scoring function when moving to a new visual domain.
      </span>
      </h2>

      <br><br>
      <h3 class="title is-4 ">Codebook Encodes Only the Most Important Information to the Task</h3>
      <p>
        We conduct an analysis (through <b>linear probing</b>, <b>GradCAM attention visualization</b>, and
        <b>nearest-neighbor retrieval</b>) to explore the information encapsulated within our bottlenecked
        representations after training for <i>Object Goal Navigation</i> task. The results show that our codebook-bottlenecked
        representations effectively exclude information related to distracting visual cues and object categories
        other than the specified goal while only concentrating on the target object and encoding better information
        about <i>object goal visibility</i> and <i>proximity</i> to the agent.
      </p>

      <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/grad_cam.png" class="interpolation-image" alt="Interpolate start reference image." style="width: 70%;" />
      </div>

      <h2 class="subtitle has-text-centered">
          <span class="dnerf2"><b>GradCAM Attention Visualization.</b> While <a
            href="https://github.com/allenai/embodied-clip">EmbCLIP</a>
            ObjectNav agent is distracted by
            different objects and other visual cues even though the target object is visible in the frame, the codebook
            module helps the agent to effectively ignore such distractions and only focus on the object goal.
      </span>
      </h2>
      <br>

      <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/nn4.png" class="interpolation-image" alt="Interpolate start reference image." />
      </div>
      <h2 class="subtitle has-text-centered">
          <span class="dnerf2"><b>Nearest-Neighbor Retrieval in the Goal-Conditioned Embedding Space.</b>
            The 4 examples show that EmbCLIP-Codebook prioritizes <i>task semantics</i> while
            <a href="https://github.com/allenai/embodied-clip">EmbCLIP</a> focuses on <i>scene semantics</i>.
            In the top row, our nearest neighbors are based on <i>object goal visibility</i> and
            <i>goal proximity</i> to the agent whereas EmbCLIP nearest neighbors are based on the overall semantics
            of the scene (tables in the left or toilets far away).
            In the bottom row, our nearest neighbors favor the overall scene layout whereas EmbCLIP mostly focuses
            on colors and appearances.
      </span>
      </h2>

      <br><br>
      <h3 class="title is-4 ">Our Agent Explores More Effectively and Travels in Smoother Trajectories</h3>
      <p>
        We conduct a quantitative and qualitative analysis to compare the agent's behavior. The <b>Curvature</b> and
        <b>Success Weighted by Episode Length (SEL)</b> metrics show that our agent explores more effectively and travels
        in much smoother paths. Excessive rotations and sudden changes in direction can lead to increased energy
        consumption and increase the chances of collisions with other objects. Lower <b>SEL</b> achieved by our agent
        shows that we can find the target object in much fewer steps. The qualitative examples below show that the
        baseline agent performs lots of redundant rotations.

      </p>



<!--      <p>Overview of EmbCLIP-Codebook.</p>-->
<!--      <h2 class="title is-3">Results</h2>-->
<!--      <p>Results comparing to baselines</p>-->
<!--      <img src="media/fig3.png" class="interpolation-image" alt="Interpolate start reference image." />-->
<!--      <p>Results across other benchmark</p>-->
<!--      <img src="media/fig4.png" class="interpolation-image" alt="Interpolate start reference image." />-->

<!--      <p>Nearest-Neighbor Retrieval in the Goal-Conditioned Embedding Space</p>-->
<!--      <img src="media/fig6.png" class="interpolation-image" alt="Interpolate start reference image." />-->
<!--      <p>Examples of the navigation paths</p>-->
<!--      <img src="media/top_down_maps2.png" class="interpolation-image" alt="Interpolate start reference image." />-->
    </div>
  </div>
</section>

      <br>

<section class="colored-section">
  <div class="rows is-centered">
    <div class="container">
    <div class="columns is-vcentered is-centered">
      <div class="image-container">
        <h2>
          <p style="font-size:22px;"><b>EmbCLIP-Codebook</b></p>
        </h2>
        <img src="media/procthor1_codebook.png" alt="PNG Image" class="equal-height">
        <img src="media/procthor1_codebook.gif" alt="GIF Image" class="equal-height">
<!--        <h2 class="subtitle has-text-centered">-->
<!--          <span class="dnerf2"><b>EmbCLIP-Codebook (Ours)</b></span>-->
<!--        </h2>-->
      </div>
    </div>
    <div class="columns is-vcentered is-centered">
      <div class="image-container">
        <h2>
          <p style="font-size:22px;"><b>EmbCLIP</b></p>
        </h2>
        <img src="media/procthor1_baseline.png" alt="PNG Image" class="equal-height">
        <img src="media/procthor1_baseline.gif" alt="GIF Image" class="equal-height">
<!--        <h2 class="subtitle has-text-centered">-->
<!--          <span class="dnerf2">EmbCLIP</span>-->
<!--        </h2>-->
      </div>
    </div>
    </div>
  </div>
</section>

      <br><br>


<section class="colored-section">
  <div class="rows is-centered">
    <div class="container">
    <div class="columns is-vcentered is-centered">
      <div class="image-container">
        <h2>
          <p style="font-size:22px;"><b>EmbCLIP-Codebook</b></p>
        </h2>
        <img src="media/procthor5_codebook.png" alt="PNG Image" class="equal-height">
        <img src="media/procthor5_codebook.gif" alt="GIF Image" class="equal-height">
<!--        <h2 class="subtitle has-text-centered">-->
<!--          <span class="dnerf2"><b>EmbCLIP-Codebook (Ours)</b></span>-->
<!--        </h2>-->
      </div>
    </div>
    <div class="columns is-vcentered is-centered">
      <div class="image-container">
        <h2>
          <p style="font-size:22px;"><b>EmbCLIP</b></p>
        </h2>
        <img src="media/procthor5_baseline.png" alt="PNG Image" class="equal-height">
        <img src="media/procthor5_baseline.gif" alt="GIF Image" class="equal-height">
<!--        <h2 class="subtitle has-text-centered">-->
<!--          <span class="dnerf2">EmbCLIP</span>-->
<!--        </h2>-->
      </div>
    </div>
    </div>
  </div>
</section>

      <br><br>

<section class="colored-section">
  <div class="rows is-centered">
    <div class="container">
    <div class="columns is-vcentered is-centered">
      <div class="image-container">
        <h2>
          <p style="font-size:22px;"><b>EmbCLIP-Codebook</b></p>
        </h2>
        <img src="media/procthor4_codebook.png" alt="PNG Image" class="equal-height">
        <img src="media/procthor4_codebook.gif" alt="GIF Image" class="equal-height">
<!--        <h2 class="subtitle has-text-centered">-->
<!--          <span class="dnerf2"><b>EmbCLIP-Codebook (Ours)</b></span>-->
<!--        </h2>-->
      </div>
    </div>
    <div class="columns is-vcentered is-centered">
      <div class="image-container">
        <h2>
          <p style="font-size:22px;"><b>EmbCLIP</b></p>
        </h2>
        <img src="media/procthor4_baseline.png" alt="PNG Image" class="equal-height">
        <img src="media/procthor4_baseline.gif" alt="GIF Image" class="equal-height">

<!--        <h2 class="subtitle">-->
<!--          <p class="is-bold"><b>End Frame</b></p>-->
<!--&lt;!&ndash;          <span class="dnerf2">EmbCLIP</span>&ndash;&gt;-->
<!--        </h2>-->
      </div>
    </div>
    </div>
  </div>
</section>

<h2 class="subtitle has-text-centered">
</br>
  Our agent explores the environment much more effectively and travels in much smoother trajectories. Whereas the
  <a href="https://github.com/allenai/embodied-clip">EmbCLIP</a> baseline agent makes many redundant rotations.
</h2>
<br>

<section>
  <div class="rows">
    <div class="row is-full-width">
      <h2 class="title is-3"><span >BibTeX</span></h2>
      <pre><code>@article{eftekhar2023selective,
  title={Selective Visual Representations Improve Convergence and Generalization for Embodied AI},
  author={Eftekhar, Ainaz and Zeng, Kuo-Hao and Duan, Jiafei and Farhadi, Ali and Kembhavi, Ani and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2311.04193},
  year={2023}
}
</code></pre>

    </div>
  </div>
</section>


<br><br>


<!-- <section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3 centered-text">Ablation Studies</h2>
      <p class="centered-text">We provide an analysis of the NEWTON dataset, focusing on potential ways of leveraging \dataset to enhance model performance in a physical reasoning context, and examining the consistency of LLMs with regard to model size, question polarity, and answer positioning. </p>
          <img src="media/8.png" class="interpolation-image"
           alt="Interpolate start reference image." />
          </br>


        <br/>





    </div>

  </div>

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2023newton,
  title     = {NEWTON: Are Large Language Models Capable of Physical Reasoning?},
  author    = {Wang, Yi Ru and Duan, Jiafei and Fox, Dieter and Srinivasa, Siddhartha,
  booktitle = {arXiv preprint arXiv:2310.07018},
  year      = {2023},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            This website is based on the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies website template</a>,
            which is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons
            Attribution-ShareAlike 4.0 International License</a> .
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
